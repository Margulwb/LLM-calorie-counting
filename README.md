# ğŸ‹ï¸ LLM Weight Loss Expert - AI-Powered Diet & Fitness Assistant

Kompletna aplikacja produkcyjna - LLM specjalista od odchudzania, liczenia kalorii, planowania diet i treningÃ³w. Wykorzystuje Ollama + Mistral 7B z systemem RAG do udzielania ekspertowych porad.

## âœ¨ Funkcje

### Backend API (Flask)
- ğŸ’¬ **Diet Expert Chat** - Pytaj eksperta o dietÄ™, trening, suplementacjÄ™
- ğŸ“Š **Kalkulator Kalorii** - Obliczanie BMR, TDEE, deficytu kalorycznego
- ğŸ½ï¸ **Generator PlanÃ³w PosiÅ‚kÃ³w** - Spersonalizowane plany Å¼ywieniowe
- ğŸ’ª **Generator PlanÃ³w Treningowych** - SiÅ‚a, cardio, lub poÅ‚Ä…czenie
- ğŸ“ˆ **Analiza PostÄ™pÃ³w** - Co robiÄ‡ gdy waga stoi, optymalizacja

### LLM + RAG System
- ğŸ¤– **Ollama + Mistral 7B** - Lokalny model jÄ™zykowy
- ğŸ“š **Baza Wiedzy** - Kompleksowa wiedza o odchudzaniu
- ğŸ” **RAG (Retrieval Augmented Generation)** - Precyzyjne odpowiedzi
- ğŸ§  **LangChain + ChromaDB** - Vector store dla wiedzy

### Frontend
- ğŸ¨ **Prosty UI** - Interfejs do testowania wszystkich funkcji
- ğŸ“± **Responsive** - DziaÅ‚a na desktop i mobile
- âš¡ **Real-time** - Natychmiastowe odpowiedzi

## ğŸ“‹ Wymagania

### System
- Rocky Linux 9 / RHEL 9 / CentOS Stream 9 (lub Ubuntu/Debian)
- Python 3.9+
- 8GB RAM minimum (16GB zalecane dla Mistral 7B)
- ~10GB wolnego miejsca (model + dependencies)

### Wymagane pakiety
- curl
- gcc, g++, make
- git

## ğŸš€ Instalacja

### Opcja 1: Automatyczna instalacja (Rocky Linux 9)

```bash
# Sklonuj repozytorium
git clone https://github.com/Margulwb/LLM-calorie-counting.git
cd LLM-calorie-counting

# Uruchom skrypt instalacyjny (wymaga sudo)
sudo bash setup_rocky9.sh
```

Skrypt automatycznie:
- Zainstaluje Python 3.9+ i zaleÅ¼noÅ›ci systemowe
- Zainstaluje Ollama
- Pobierze model Mistral 7B
- Utworzy Å›rodowisko wirtualne Python
- Zainstaluje zaleÅ¼noÅ›ci Python
- Skonfiguruje systemd service
- Zainicjuje bazÄ™ wiedzy

### Opcja 2: Docker

```bash
# Zbuduj i uruchom
cd docker
docker-compose up -d

# Poczekaj aÅ¼ Ollama pobierze model (pierwsze uruchomienie)
docker-compose logs -f ollama

# SprawdÅº status
docker-compose ps
```

API dostÄ™pne pod: `http://localhost:5000`
Frontend dostÄ™pny pod: `http://localhost:8080`

### Opcja 3: Manualna instalacja

```bash
# 1. Zainstaluj Python 3.9+
sudo dnf install python3.9 python3.9-pip python3.9-devel -y

# 2. Zainstaluj Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# 3. Uruchom Ollama
sudo systemctl start ollama
sudo systemctl enable ollama

# 4. Pobierz model Mistral
ollama pull mistral

# 5. Sklonuj repozytorium
git clone https://github.com/Margulwb/LLM-calorie-counting.git
cd LLM-calorie-counting

# 6. UtwÃ³rz Å›rodowisko wirtualne
python3.9 -m venv venv
source venv/bin/activate

# 7. Zainstaluj zaleÅ¼noÅ›ci
pip install --upgrade pip
pip install -r requirements.txt

# 8. Skopiuj i dostosuj konfiguracjÄ™
cp .env.example .env
# Edytuj .env jeÅ›li potrzeba

# 9. Uruchom aplikacjÄ™
python backend/app.py
```

## ğŸ¯ UÅ¼ycie

### Uruchamianie serwera

**Z systemd (po automatycznej instalacji):**
```bash
# Uruchom
sudo systemctl start llm-calorie-app

# Status
sudo systemctl status llm-calorie-app

# Logi
sudo journalctl -u llm-calorie-app -f

# Stop
sudo systemctl stop llm-calorie-app
```

**Manualnie:**
```bash
cd /opt/llm-calorie-app  # lub Å›cieÅ¼ka do repo
source venv/bin/activate
python backend/app.py
```

### Testowanie API

**Health Check:**
```bash
curl http://localhost:5000/health
```

**Pytanie do eksperta:**
```bash
curl -X POST http://localhost:5000/api/diet-expert \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Ile biaÅ‚ka powinienem jeÅ›Ä‡ przy redukcji?",
    "context": {
      "weight": 80,
      "activity_level": "moderate"
    }
  }'
```

**Obliczenie kalorii:**
```bash
curl -X POST http://localhost:5000/api/calculate-deficit \
  -H "Content-Type: application/json" \
  -d '{
    "weight": 80,
    "height": 180,
    "age": 30,
    "gender": "male",
    "activity_level": "moderate",
    "goal": "weight_loss"
  }'
```

**WiÄ™cej przykÅ‚adÃ³w:** Zobacz [API_DOCUMENTATION.md](API_DOCUMENTATION.md)

### Interfejs Web

OtwÃ³rz w przeglÄ…darce: `http://localhost:5000` lub `http://your-server-ip:5000`

Alternatywnie otwÃ³rz `frontend/index.html` bezpoÅ›rednio w przeglÄ…darce.

## ğŸ“š Dokumentacja

- **[API Documentation](API_DOCUMENTATION.md)** - PeÅ‚na dokumentacja API z przykÅ‚adami
- **Knowledge Base** - Baza wiedzy w `knowledge_base/`:
  - `calories_macros.md` - Kalorie i makroskÅ‚adniki
  - `caloric_deficit.md` - Deficyty kaloryczne i redukcja
  - `strength_vs_cardio.md` - Treningi siÅ‚owe vs cardio
  - `psychology.md` - Psychologia odchudzania
  - `supplements.md` - Suplementacja

## ğŸ—ï¸ Struktura Projektu

```
LLM-calorie-counting/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py                 # GÅ‚Ã³wna aplikacja Flask
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ llm_service.py     # Serwis LLM + RAG
â”‚       â””â”€â”€ calculator_service.py  # Kalkulatory BMR/TDEE
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ index.html             # Interfejs uÅ¼ytkownika
â”œâ”€â”€ knowledge_base/
â”‚   â”œâ”€â”€ calories_macros.md     # Baza wiedzy
â”‚   â”œâ”€â”€ caloric_deficit.md
â”‚   â”œâ”€â”€ strength_vs_cardio.md
â”‚   â”œâ”€â”€ psychology.md
â”‚   â””â”€â”€ supplements.md
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ llm-calorie-app.service  # Systemd service
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ nginx.conf
â”œâ”€â”€ tests/                     # Testy (do dodania)
â”œâ”€â”€ requirements.txt           # ZaleÅ¼noÅ›ci Python
â”œâ”€â”€ setup_rocky9.sh           # Skrypt instalacyjny
â”œâ”€â”€ .env.example              # PrzykÅ‚adowa konfiguracja
â”œâ”€â”€ .gitignore
â”œâ”€â”€ API_DOCUMENTATION.md
â””â”€â”€ README.md
```

## ğŸ”§ Konfiguracja

### Zmienne Å›rodowiskowe (.env)

```bash
# Flask
FLASK_ENV=production
PORT=5000

# Ollama
OLLAMA_MODEL=mistral
OLLAMA_HOST=http://localhost:11434

# Logi
LOG_LEVEL=INFO
```

### Zmiana modelu LLM

MoÅ¼esz uÅ¼yÄ‡ innych modeli wspieranych przez Ollama:

```bash
# Pobierz inny model
ollama pull llama2
# lub
ollama pull codellama

# Zaktualizuj .env
OLLAMA_MODEL=llama2
```

## ğŸ” BezpieczeÅ„stwo

### Dla produkcji:
1. **Firewall:** Ogranicz dostÄ™p do portu 5000
2. **Reverse Proxy:** UÅ¼yj Nginx/Apache z SSL
3. **Authentication:** Dodaj API keys lub OAuth2
4. **Rate Limiting:** Ogranicz liczbÄ™ requestÃ³w
5. **CORS:** Skonfiguruj konkretne dozwolone originy

### PrzykÅ‚ad Nginx z SSL:
```nginx
server {
    listen 443 ssl http2;
    server_name your-domain.com;

    ssl_certificate /path/to/cert.pem;
    ssl_certificate_key /path/to/key.pem;

    location / {
        proxy_pass http://localhost:5000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

## ğŸ“Š Monitorowanie

### Logi systemd:
```bash
# Ostatnie logi
sudo journalctl -u llm-calorie-app -n 100

# Logi na Å¼ywo
sudo journalctl -u llm-calorie-app -f

# Logi z ostatniej godziny
sudo journalctl -u llm-calorie-app --since "1 hour ago"
```

### Status usÅ‚ug:
```bash
# Status aplikacji
sudo systemctl status llm-calorie-app

# Status Ollama
sudo systemctl status ollama

# SprawdÅº czy Ollama dziaÅ‚a
curl http://localhost:11434/api/version
```

## ğŸ› Troubleshooting

### Ollama nie odpowiada
```bash
# Restart Ollama
sudo systemctl restart ollama

# SprawdÅº logi
sudo journalctl -u ollama -n 50
```

### Model nie zostaÅ‚ pobrany
```bash
# SprawdÅº dostÄ™pne modele
ollama list

# Pobierz Mistral
ollama pull mistral
```

### BÅ‚Ä™dy z ChromaDB
```bash
# UsuÅ„ i zainicjuj ponownie
rm -rf chroma_db/
python -c "from backend.services.llm_service import LLMService; LLMService()"
```

### Porty juÅ¼ zajÄ™te
```bash
# SprawdÅº co uÅ¼ywa portu 5000
sudo lsof -i :5000

# ZmieÅ„ port w .env
PORT=5001
```

## ğŸ§ª Testowanie

```bash
# Uruchom testy (gdy zostanÄ… dodane)
source venv/bin/activate
pytest tests/

# Lub manualnie testuj endpoints
python tests/manual_test.py
```

## ğŸ“ˆ Performance

### Typowe czasy odpowiedzi:
- Health check: ~10ms
- Kalkulacje: ~50ms
- Generowanie z LLM: 2-10s (zaleÅ¼nie od modelu i hardware)
- Pierwsze zapytanie (inicjalizacja): ~30s

### Optymalizacja:
- UÅ¼yj GPU dla Ollama (znacznie szybsze)
- UÅ¼yj mniejszego modelu (mistral:7b-instruct-q4_0)
- ZwiÄ™ksz RAM dla cache'owania modelu

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open Pull Request

## ğŸ“ License

Ten projekt jest otwarty na uÅ¼ytek osobisty i edukacyjny.

## ğŸ‘¨â€ğŸ’» Autor

Stworzony dla spoÅ‚ecznoÅ›ci fitness i health tech.

## ğŸ™ PodziÄ™kowania

- **Ollama** - Lokalny LLM runtime
- **Mistral AI** - Model jÄ™zykowy
- **LangChain** - Framework RAG
- **Flask** - Web framework

## ğŸ“ Wsparcie

JeÅ›li masz problemy:
1. SprawdÅº [Issues](https://github.com/Margulwb/LLM-calorie-counting/issues)
2. Przeczytaj dokumentacjÄ™ API
3. SprawdÅº logi systemd
4. OtwÃ³rz nowy Issue z opisem problemu

## ğŸ”„ Aktualizacje

```bash
# Zaktualizuj kod
git pull origin main

# Zaktualizuj zaleÅ¼noÅ›ci
source venv/bin/activate
pip install -r requirements.txt --upgrade

# Restart usÅ‚ugi
sudo systemctl restart llm-calorie-app
```

---

**Status projektu:** âœ… Production Ready

**Ostatnia aktualizacja:** 2024
